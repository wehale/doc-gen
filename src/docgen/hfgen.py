import huggingface_hub as hf
import glob
import jsonlines as jsonl
import os
import logging
import util.code_utils as code_utils
import concurrent

MAX_TOKENS = 2000 # maximum number of tokens to generate

class HuggingFaceGenerator:

    LOG_PREFIX = "[hfai-gen]>"
    PBAR_COLOR = "green"

    def __init__(self, args, key, config, manager):
        self._args = args
        self._key = key
        self._config = config
        self._model = self._config['hfllm']['model']
        self._files = code_utils.get_code_files_from_glob(glob.glob(self._config['input']['doc_path']+"/**/*", recursive=True))
        self._prompts_file_str = self._config['hfllm']['doc_prompts']
        self._client = hf.InferenceClient(token=self._key)
        self._log = logging.getLogger(__name__)
        self._pbar = manager.counter(total=len(self._files)*3, desc=self.LOG_PREFIX, unit="prompts", color=self.PBAR_COLOR)

    def generate(self) -> None:
        self._log.log(code_utils.LOG_LEVEL, f"{self._config['hfllm']['name']} is generating...")
        executor = concurrent.futures.ThreadPoolExecutor(max_workers=len(self._files))
        for f in self._files:
            if not os.path.isfile(f):
                continue
            executor.submit(self._gen_markdown_file, f)
        # Wait for all the threads to complete
        executor.shutdown(wait=True)
        
    def _gen_markdown_file(self, f):
        input_file = open(f, 'r')
        input_file_split = f.split("/")
        input_file_name = input_file_split[len(input_file_split)-1]
        output_file_name = self._config['hfllm']['gen_file_prefix'] + input_file_split[len(input_file_split)-1].split(".")[0] + ".md"
        output_file_path = self._config['output']['doc_path']+"/"+output_file_name
        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
        open(output_file_path, 'w').close() #clear output file
        output_file = open(self._config['output']['doc_path']+"/"+output_file_name, 'a') # append the output file
        output_file.write("\n" + "# "+ self._config['hfllm']['description_prefix'] + ": " + input_file_name + "\n")
        code_file_str_for_prompt = "Consider the following code between the markers <start_code> and <end_code>\n\n<start_code>\n\n" \
            + input_file.read() + "\n\n<end_code>\n\n"
        ps = jsonl.open(self._prompts_file_str)
        for prompt in ps:
            message = code_file_str_for_prompt + prompt['description']
            response = self._client.chat_completion(messages=[{"role": "user", "content": message}], max_tokens=MAX_TOKENS)
            output_file.write("\n" + "## "+ prompt['title']+ ": " + input_file_name + "\n")
            self._log.log(code_utils.LOG_LEVEL, response.choices[0].message.content)
            self._pbar.update()
            output_file.write(response.choices[0].message.content)
            output_file.write("\n")
            output_file.write("\n" + "(Generated by "+ self._config['author'] + 
                        " using " + self._config['hfllm']['name'] + " " + 
                        self._config['hfllm']['model'] +")\n")
        input_file.close()
        output_file.close()
        